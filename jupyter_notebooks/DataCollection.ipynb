{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Data Collection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Fetch the image classification dataset CIFAR-10 from Kaggle and save it as raw data.\n",
        "* Inspect the image data.\n",
        "* Split into train, test and validation sets.\n",
        "\n",
        "## Inputs\n",
        "Write here which data or information you need to run the notebook \n",
        "* [https://www.kaggle.com/competitions/cifar-10/overview](https://www.kaggle.com/competitions/cifar-10/overview)\n",
        "* Kaggle JSON file for authentication.\n",
        "\n",
        "## Outputs\n",
        "output/train/\n",
        "output/test/\n",
        "output/trainLabels.csv\n",
        "\n",
        "## Overview\n",
        "Problem The e-commerce platform aims to improve its product categorization by developing a machine learning model that can classify images into one of 10 categories with high accuracy. To prototype this solution, the company will use a subset of the CIFAR-10 dataset, which contains a large number of images, to ensure compatibility with deployment constraints on platforms like Heroku and GitHub.\n",
        "\n",
        "**Data:**\n",
        "\n",
        "- Training Data: The CIFAR-10 dataset originally includes 50,000 images across 10 categories. For this project, a reduced subset will be used to fit within the storage and performance limits of Heroku and GitHub. Specifically, the dataset will be trimmed to a smaller number of images per category to streamline processing.\n",
        "\n",
        "- Testing Data: The CIFAR-10 dataset includes 10,000 images for testing. Similarly, this set will be reduced to ensure it is manageable within the deployment constraints. The reduced test set will be used to evaluate the model's performance and accuracy.\n",
        "\n",
        "**Important Notes:**\n",
        "- Dataset Adjustment: To make the dataset compatible with deployment constraints, a subset of images from both the training and testing sets will be selected. This will involve removing a portion of the images while retaining a representative sample of each category.\n",
        "\n",
        "- Evaluation: Despite the reduced dataset, the goal remains to achieve high accuracy in classifying images into the correct categories. The adjusted training and testing sets will be used to develop and assess the model’s performance effectively.\n",
        "\n",
        "**Quick Recap:**\n",
        "- /train - A smaller subset of 3500 images of the original 50,000 images from the training set.\n",
        "- /test - A smaller of subset 1000 images of the subset of the original 50.000 images from the training set.\n",
        "- /validation - A smaller subset of 500 images of the original 50.000 images from the training set.\n",
        "- trainLabels.csv/ - The CIFAR-10 dataset’s 10 predefined categories will still be used, but with a reduced number of images to fit deployment constraints.\n",
        "\n",
        "This adjusted approach ensures that the model can be developed, tested, and deployed efficiently while meeting the project’s business requirements.\n",
        "\n",
        "## Additional Comments TBC\n",
        "* 290,000 junk images in the test set.\n",
        "* Trivial modifications to the official 10,000 test images to prevent looking them up by file hash.\n",
        "* **We have only used the train set and divided these images into test, train and validation sets to limit the size of the dataset**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Install \n",
        "Install requirements, import libraries, and set variable DatasetFolder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -r ../requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import py7zr\n",
        "import shutil\n",
        "import random\n",
        "import joblib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "Change working directory to root project folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "print('Current folder: ' + current_dir)\n",
        "os.chdir(os.path.dirname(current_dir))\n",
        "current_dir = os.getcwd()\n",
        "print('New folder: ' + current_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fetch Data from Kaggle and Extract Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Install Kaggle, configure the directory, and set permissions for the Kaggle authentication JSON.\n",
        "* Download the Kaggle dataset.\n",
        "* Unzip the file, extract .7z files and delete the unused files and kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install kaggle==1.5.12\n",
        "print('Requirements installed.')\n",
        "\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()\n",
        "! chmod 600 kaggle.json\n",
        "print('Directory configured and permissions set.')\n",
        "\n",
        "DatasetFolder = 'inputs/cifar10_dataset_small'\n",
        "\n",
        "! kaggle competitions download -c cifar-10 -p {DatasetFolder}\n",
        "\n",
        "# Extract ZIP file\n",
        "zip_file_path = os.path.join(DatasetFolder, 'cifar-10.zip')\n",
        "if os.path.exists(zip_file_path):\n",
        "    print('Extracting ZIP file...')\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(DatasetFolder)\n",
        "    print('ZIP file extraction complete.')\n",
        "else:\n",
        "    print(f\"File {zip_file_path} not found.\")\n",
        "\n",
        "# Extract train.7z file\n",
        "train_7z_path = os.path.join(DatasetFolder, 'train.7z')\n",
        "if os.path.exists(train_7z_path):\n",
        "    print('Extracting train.7z set...')\n",
        "    with py7zr.SevenZipFile(train_7z_path, 'r') as archive:\n",
        "        archive.extractall(DatasetFolder)\n",
        "    print('train.7z extracted.')\n",
        "else:\n",
        "    print(f\"File {train_7z_path} not found.\")\n",
        "\n",
        "# Delete original files after successful extraction\n",
        "files_to_remove = [\n",
        "    os.path.join(DatasetFolder, 'cifar-10.zip'),\n",
        "    os.path.join(DatasetFolder, 'test.7z'),\n",
        "    os.path.join(DatasetFolder, 'train.7z'),\n",
        "    os.path.join(DatasetFolder, 'sampleSubmission.csv')\n",
        "]\n",
        "for file_path in files_to_remove:\n",
        "    try:\n",
        "        os.remove(file_path)\n",
        "        print(f\"Removed {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error removing {file_path}: {str(e)}\")\n",
        "\n",
        "print('Unused files deleted.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Divide Images Into Respective Class Directories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are sampling 500 images per class for this project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "\n",
        "def load_filename_to_class_map(csv_file_path):\n",
        "    \"\"\"\n",
        "    Load a mapping from a CSV file that contains image filenames and their corresponding classes.\n",
        "    \n",
        "    Args:\n",
        "        csv_file_path (str): Path to the CSV file.\n",
        "        \n",
        "    Returns:\n",
        "        dict: A dictionary mapping filenames to class names.\n",
        "    \"\"\"\n",
        "    filename_to_class_map = {}\n",
        "    \n",
        "    with open(csv_file_path, mode='r') as file:\n",
        "        reader = csv.DictReader(file)\n",
        "        for row in reader:\n",
        "            filename = row['id'] + '.png'  # CSV has an 'id' column for image number\n",
        "            label = row['label']           # CSV has a 'label' column for class name\n",
        "            filename_to_class_map[filename] = label\n",
        "    \n",
        "    return filename_to_class_map\n",
        "\n",
        "def split_images(dataset_root_dir, classes, filename_to_class_map, total_samples=500):\n",
        "    \"\"\"\n",
        "    Splits images from the source directory into class-specific directories,\n",
        "    ensuring each class has exactly 500 images.\n",
        "\n",
        "    Args:\n",
        "        dataset_root_dir (str): Path to the dataset directory.\n",
        "        classes (list): List of class names to split (e.g., ['airplane', 'automobile', 'bird', ...]).\n",
        "        filename_to_class_map (dict): Dictionary mapping filenames to class names.\n",
        "        total_samples (int): Number of images to sample for each class (default is 500).\n",
        "    \"\"\"\n",
        "    # Source directory\n",
        "    source_dir = os.path.join(dataset_root_dir, 'train')\n",
        "\n",
        "    if not os.path.exists(source_dir):\n",
        "        print(f\"Source directory {source_dir} does not exist!\")\n",
        "        return\n",
        "\n",
        "    # Create class-specific directories within dataset_root_dir\n",
        "    for cls in classes:\n",
        "        class_dir = os.path.join(dataset_root_dir, cls)\n",
        "        os.makedirs(class_dir, exist_ok=True)\n",
        "        print(f\"Created class folder: {class_dir}\")  \n",
        "\n",
        "    # Organize files by class\n",
        "    class_files = {cls: [] for cls in classes}\n",
        "    for file_name in os.listdir(source_dir):\n",
        "        if file_name.endswith('.png'):\n",
        "            cls = filename_to_class_map.get(file_name)\n",
        "            if cls:\n",
        "                file_path = os.path.join(source_dir, file_name)\n",
        "                class_files[cls].append(file_path)\n",
        "\n",
        "    # Calculate the number of samples per class\n",
        "    samples_per_class = min(total_samples, len(class_files[classes[0]]))\n",
        "\n",
        "    # Randomly sample images for each class and move them to their respective folders\n",
        "    for cls, files in class_files.items():\n",
        "        selected_files = random.sample(files, samples_per_class)\n",
        "        for file_path in selected_files:\n",
        "            dest_path = os.path.join(dataset_root_dir, cls, os.path.basename(file_path))\n",
        "            try:\n",
        "                shutil.move(file_path, dest_path)\n",
        "            except Exception as e:\n",
        "                print(f\"Error moving file {file_path} to {dest_path}: {str(e)}\")\n",
        "\n",
        "    # Delete the original train directory after moving all files\n",
        "    try:\n",
        "        shutil.rmtree(source_dir)\n",
        "        print(\"\\nOriginal train directory deleted successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError deleting original train directory: {str(e)}\")\n",
        "\n",
        "    # Print when the process is done\n",
        "    print(\"\\nAll files have been moved to their respective class folders.\")\n",
        "\n",
        "# Load the filename to class map\n",
        "csv_file_path = 'inputs/cifar10_dataset_small/trainLabels.csv'\n",
        "filename_to_class_map = load_filename_to_class_map(csv_file_path)\n",
        "\n",
        "# Class names\n",
        "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "\n",
        "dataset_root_dir = 'inputs/cifar10_dataset_small'\n",
        "\n",
        "# Call the function to split images\n",
        "split_images(dataset_root_dir=dataset_root_dir, classes=classes, filename_to_class_map=filename_to_class_map, total_samples=500)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clean Up - Remove Non-Images from Class Directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def remove_non_image_file(dataset_root_dir):\n",
        "    \"\"\"\n",
        "    Removes non-image files from the dataset directory, ensuring only image files remain.\n",
        "    \n",
        "    Args:\n",
        "        dataset_root_dir (str): The root directory containing subdirectories of image classes.\n",
        "    \"\"\"\n",
        "    # Define valid image extensions\n",
        "    image_extensions = ('.png', '.jpg', '.jpeg')  \n",
        "\n",
        "    # List all subdirectories inside the dataset root\n",
        "    folders = os.listdir(dataset_root_dir)\n",
        "\n",
        "    for folder in folders:\n",
        "        folder_path = os.path.join(dataset_root_dir, folder)\n",
        "\n",
        "        if os.path.isdir(folder_path):\n",
        "            files = os.listdir(folder_path)\n",
        "\n",
        "            image_count = 0\n",
        "            non_image_count = 0\n",
        "\n",
        "            for given_file in files:\n",
        "                file_path = os.path.join(folder_path, given_file)\n",
        "\n",
        "                # Check if file doesn't have a valid image extension\n",
        "                if not given_file.lower().endswith(image_extensions):\n",
        "                    # Remove non-image file\n",
        "                    os.remove(file_path)\n",
        "                    non_image_count += 1\n",
        "                else:\n",
        "                    image_count += 1\n",
        "\n",
        "            # Report the number of images and non-images in the folder\n",
        "            print(f\"Folder: {folder} - contains {image_count} image files\")\n",
        "            print(f\"Folder: {folder} - removed {non_image_count} non-image files\")\n",
        "\n",
        "remove_non_image_file(dataset_root_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspect the First 10 Labels Available in the trainLabels.csv file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(f\"{dataset_root_dir}/trainLabels.csv\")\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By checking our DataFrame, we can see that:\n",
        "\n",
        "* There are 10 unique labels.\n",
        "* We noticed Labels is a categorical variables. We will replace/convert it to an integer as the ML model requires numeric variables.\n",
        "* The CSV file can act as a complete record of all the original labels and their corresponding image IDs, so we keep that file untouched."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['label'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Checking that the class directories includes 500 images each:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_folders = os.listdir(dataset_root_dir)\n",
        "\n",
        "for class_folder in class_folders:\n",
        "    class_folder_path = os.path.join(dataset_root_dir, class_folder)\n",
        "    \n",
        "\n",
        "    if os.path.exists(class_folder_path) and os.path.isdir(class_folder_path):\n",
        "\n",
        "        files = os.listdir(class_folder_path)\n",
        "        print(f\"Total number of files in '{class_folder}': {len(files)}\")\n",
        "    else:\n",
        "        print(f\"The path {class_folder_path} does not exist or is not a directory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Split Dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Split dataset into train (70%), validation (10%) and test (20%) sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_train_validation_test_images(dataset_root_dir, train_ratio=0.7, validation_ratio=0.1, test_ratio=0.2):\n",
        "    \"\"\"\n",
        "    Splits dataset into train, validation, and test sets, and moves images into respective folders.\n",
        "    \n",
        "    Args:\n",
        "        dataset_root_dir (str): Root directory containing class folders with images.\n",
        "        train_ratio (float): Ratio of the dataset to be used for training.\n",
        "        validation_ratio (float): Ratio of the dataset to be used for validation.\n",
        "        test_ratio (float): Ratio of the dataset to be used for testing.\n",
        "    \"\"\"\n",
        "    # Check if the sum of ratios is equal to 1.0\n",
        "    if train_ratio + validation_ratio + test_ratio != 1.0:\n",
        "        raise ValueError(\"The sum of train_ratio, validation_ratio, and test_ratio should equal 1.0.\")\n",
        "    \n",
        "    classes = [folder for folder in os.listdir(dataset_root_dir) if os.path.isdir(os.path.join(dataset_root_dir, folder)) \n",
        "               and folder not in ['train', 'validation', 'test']]\n",
        "    \n",
        "    for split in ['train', 'validation', 'test']:\n",
        "        # Create train, validation, and test directories if they don't exist\n",
        "        split_dir = os.path.join(dataset_root_dir, split)\n",
        "        os.makedirs(split_dir, exist_ok=True)\n",
        "        for class_name in classes:\n",
        "            os.makedirs(os.path.join(split_dir, class_name), exist_ok=True)\n",
        "\n",
        "    for class_name in classes:\n",
        "        # Get the list of all images for this class\n",
        "        class_dir = os.path.join(dataset_root_dir, class_name)\n",
        "        images = os.listdir(class_dir)\n",
        "\n",
        "        random.shuffle(images)\n",
        "        \n",
        "        # Calculate number of images for each split\n",
        "        total_images = len(images)\n",
        "        train_count = int(total_images * train_ratio)\n",
        "        validation_count = int(total_images * validation_ratio)\n",
        "        test_count = total_images - train_count - validation_count  # Remaining goes to test\n",
        "\n",
        "        train_set = images[:train_count]\n",
        "        validation_set = images[train_count:train_count + validation_count]\n",
        "        test_set = images[train_count + validation_count:]\n",
        "        \n",
        "        # Move images to the respective directories\n",
        "        for image in train_set:\n",
        "            shutil.move(os.path.join(class_dir, image), os.path.join(dataset_root_dir, 'train', class_name, image))\n",
        "        for image in validation_set:\n",
        "            shutil.move(os.path.join(class_dir, image), os.path.join(dataset_root_dir, 'validation', class_name, image))\n",
        "        for image in test_set:\n",
        "            shutil.move(os.path.join(class_dir, image), os.path.join(dataset_root_dir, 'test', class_name, image))\n",
        "\n",
        "        print(f\"Class '{class_name}' -> Train: {len(train_set)}, Validation: {len(validation_set)}, Test: {len(test_set)}\")\n",
        "\n",
        "    # Remove the original class directories if all images are moved\n",
        "    for class_name in classes:\n",
        "        class_dir = os.path.join(dataset_root_dir, class_name)\n",
        "        if os.path.exists(class_dir) and os.path.isdir(class_dir):\n",
        "            try:\n",
        "                shutil.rmtree(class_dir)\n",
        "            except Exception as e:\n",
        "                print(f\"Error removing directory {class_dir}: {e}\")\n",
        "\n",
        "split_train_validation_test_images(dataset_root_dir, train_ratio=0.7, validation_ratio=0.1, test_ratio=0.2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load and Inspect Image Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def display_images(dataset_dir, num_images=5):\n",
        "    \"\"\"\n",
        "    Displays a specified number of random images from the given directory and its subdirectories.\n",
        "    \n",
        "    Args:\n",
        "        dataset_dir (str): Path to the directory containing class subdirectories with images.\n",
        "        num_images (int): Number of images to display.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(dataset_dir):\n",
        "        print(f\"Error: The directory '{dataset_dir}' does not exist.\")\n",
        "        return\n",
        "\n",
        "    classes = [d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))]\n",
        "    \n",
        "    image_paths = []\n",
        "    \n",
        "    # Collect image paths from each class subdirectory\n",
        "    for class_name in classes:\n",
        "        class_dir = os.path.join(dataset_dir, class_name)\n",
        "        files = [f for f in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, f))]\n",
        "        \n",
        "        for file in files:\n",
        "            image_paths.append(os.path.join(class_dir, file))\n",
        "    \n",
        "    random.shuffle(image_paths)\n",
        "    \n",
        "    # Create subplots dynamically based on the number of images\n",
        "    if num_images > 1:\n",
        "        fig, axes = plt.subplots(num_images, 1, figsize=(10, 4 * num_images))\n",
        "    else:\n",
        "        fig, axes = plt.subplots(1, 1, figsize=(10, 4))\n",
        "        axes = [axes]  # Make axes iterable if there's only one subplot\n",
        "    \n",
        "    for i, img_path in enumerate(image_paths[:num_images]):\n",
        "        try:\n",
        "            img = Image.open(img_path)\n",
        "            axes[i].imshow(img)\n",
        "            axes[i].set_title(os.path.basename(img_path))\n",
        "            axes[i].axis('off')\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: {os.path.basename(img_path)} could not be read. Error: {e}\")\n",
        "    \n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "train_folder = os.path.join(dataset_root_dir, 'train')\n",
        "\n",
        "display_images(train_folder, num_images=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Copy random images from each class to output folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def copy_random_images(number_per_class, dataset_root_dir, output_folder, classes):\n",
        "    \"\"\"\n",
        "    Copies a specified number of random images from each class in the CIFAR-10 dataset \n",
        "    to a specified output folder.\n",
        "    \n",
        "    Args:\n",
        "        number_per_class (int): Number of images to sample from each class.\n",
        "        dataset_root_dir (str): Path to the CIFAR-10 dataset folder.\n",
        "        output_folder (str): Path to the output folder where sampled images will be stored.\n",
        "        classes (list): List of class names in the dataset.\n",
        "    \"\"\"\n",
        "    \n",
        "    validation_dir = os.path.join(dataset_root_dir, 'validation')\n",
        "    \n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    \n",
        "    for class_name in classes:\n",
        "        # Create the sample directory for the current class\n",
        "        sample_class_dir = os.path.join(output_folder, class_name)\n",
        "        os.makedirs(sample_class_dir, exist_ok=True)\n",
        "\n",
        "        # Define the class directory in the validation set\n",
        "        class_dir = os.path.join(validation_dir, class_name)\n",
        "        \n",
        "        print(f\"Checking class directory: {class_dir}\")\n",
        "\n",
        "        if not os.path.exists(class_dir):\n",
        "            print(f\"Warning: Directory for class '{class_name}' does not exist at {class_dir}.\")\n",
        "            continue\n",
        "\n",
        "        # Get a list of all images in the class directory\n",
        "        all_images = os.listdir(class_dir)\n",
        "\n",
        "        # Ensure we do not sample more images than available\n",
        "        if len(all_images) < number_per_class:\n",
        "            print(f\"Warning: Not enough images in class '{class_name}' to sample {number_per_class} images. Sampling all available images.\")\n",
        "            number_per_class = len(all_images)\n",
        "        \n",
        "        random_images = random.sample(all_images, number_per_class)\n",
        "\n",
        "        # Copy the selected images to the sample directories\n",
        "        for image in random_images:\n",
        "            src_image_path = os.path.join(class_dir, image)\n",
        "            dest_image_path = os.path.join(sample_class_dir, image)\n",
        "            shutil.copy(src_image_path, dest_image_path)\n",
        "\n",
        "    print(\"Sampling completed.\")\n",
        "\n",
        "\n",
        "output_folder = 'outputs/sample_images'\n",
        "\n",
        "copy_random_images(5, dataset_root_dir, output_folder, classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a ZIP File of the Sampled Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_zip_from_folder(folder_path, zip_filename=\"sample_images.zip\"):\n",
        "    zip_path = os.path.join(os.path.dirname(folder_path), zip_filename)\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk(folder_path):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, folder_path)\n",
        "                zipf.write(file_path, arcname)\n",
        "\n",
        "    return zip_path\n",
        "\n",
        "\n",
        "OutputFolder = 'outputs/sample_images'\n",
        "create_zip_from_folder(OutputFolder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusions And Next Steps\n",
        "\n",
        "This initial configuration prepares the files.\n",
        "By downloading the files and setting them up in the right directories we can directly access them. \n",
        "By loading the labels and images we were able to see that we are dealing with 10 different objects.\n",
        "\n",
        "The next notebook includes Data Visualization:\n",
        "\n",
        "**1. Visualize Dataset Overview:**\n",
        "* Create plots to show the distribution of images across different classes.\n",
        "* Generate a few sample images from each class to confirm the dataset’s diversity and balance.\n",
        "\n",
        "**2. Explore Data Distribution:**\n",
        "* Plot histograms or bar charts to visualize the number of images in each class.\n",
        "* Use pie charts or bar plots to display the distribution of classes in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Data collection and cleaning has finished. You can push the files to the GitHub repository and close this notebook."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
